{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "5ORFW3OkhqbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label the Images\n",
        "\n",
        "- Use [LabelImg](https://pypi.org/project/labelImg/) to annotate all the images and save them to ``train``.\n",
        "- Cut the last 100 images and their xml files and paste them to a folder called ``validate``.\n",
        "- The final tree should look like this:\n",
        "```bash\n",
        "$ tree\n",
        "|   train.ipynb\n",
        "|\n",
        "└───data\n",
        "    ├───train\n",
        "    |    ...\n",
        "    └───validate\n",
        "    |    ...\n",
        "```"
      ],
      "metadata": {
        "id": "CxQWLO_SiY8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the Environment"
      ],
      "metadata": {
        "id": "mGzXBZaXx5ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the sample_data folder\n",
        "!rm -rf sample_data"
      ],
      "metadata": {
        "id": "38mi_KHEycWP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the repository and extract the labeled images\n",
        "!wget https://github.com/Scorpions-Robotics/dl-2022/archive/refs/heads/master.zip -q &&\\\n",
        " unzip -o -q master.zip && rm -f master.zip* && cp -r dl-2022-master/data . && rm -rf dl-2022-master"
      ],
      "metadata": {
        "id": "eyXzncoJzaiX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the required packages\n",
        "!pip install tflite-support tflite-model-maker"
      ],
      "metadata": {
        "id": "jro0Oymj42_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Dp1Bo5PW4NXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Import the required packages"
      ],
      "metadata": {
        "id": "C127es8S5PHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ],
      "metadata": {
        "id": "65VlQmUy5Vuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Load the dataset\n",
        "\n",
        "- Images in `train_data` is used to train the custom object detection model.\n",
        "- Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before."
      ],
      "metadata": {
        "id": "GVT2-Ynw5u9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'data/train',\n",
        "    'data/train',\n",
        "    ['hoop']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'data/validate',\n",
        "    'data/validate',\n",
        "    ['hoop']\n",
        ")"
      ],
      "metadata": {
        "id": "Sql6J6x354Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Select a model architecture\n",
        "\n",
        "EfficientDet-Lite [0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite4 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."
      ],
      "metadata": {
        "id": "fhBiJIl26KZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spec = model_spec.get('efficientdet_lite4')"
      ],
      "metadata": {
        "id": "LoaKV1NI6YxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Train the TensorFlow model with the training data\n",
        "\n",
        "* Set `epochs = 30`, which means it will go through the training dataset 30 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
        "* Set `batch_size = 21` here so you will see that it takes 40 steps to go through the 845 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ],
      "metadata": {
        "id": "HT-2U4Cg6m6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=21, train_whole_model=True, epochs=30, validation_data=val_data)"
      ],
      "metadata": {
        "id": "M0hBPr3F8Zj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4. Evaluate the model with the validation data\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 2 steps to go through the 100 images in the validation dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ],
      "metadata": {
        "id": "xeegQAJb8sUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_data)"
      ],
      "metadata": {
        "id": "q7dw5bIK85o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Export as a TensorFlow Lite model\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Jetson Nano GPU."
      ],
      "metadata": {
        "id": "Wr2vanWp88ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.export(export_dir='.', tflite_filename='model.tflite')"
      ],
      "metadata": {
        "id": "Uk6gIekO9KHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6:  Evaluate the TensorFlow Lite model\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ],
      "metadata": {
        "id": "NmwJoybl9ObE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate_tflite('model.tflite', val_data)"
      ],
      "metadata": {
        "id": "kOJlvU8u9Yxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Download the exported model to your local machine"
      ],
      "metadata": {
        "id": "qLDxNO5o9bgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('model.tflite')"
      ],
      "metadata": {
        "id": "_qVAUR9R9zxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}